## ONNX Inference Service

### Description

Serves inference on **ONNX** models. [ONNX](https://onnx.ai/) is an open format for representing ML models that enables interoperability between different frameworks and hardware platforms. This service allows you to deploy and run models for various tasks.

For configuration and usage details, check out the [ONNX Inference Service](https://infernet-services.docs.ritual.net/reference/onnx_inference_service) documentation.
