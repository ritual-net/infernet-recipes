{
    "config": {
        "id": "onnx-inference-2.0.0",
        "image": "ritualnetwork/onnx_inference_service:2.0.0",
        "description": "ONNX Inference Service",
        "command": "--bind=0.0.0.0:3000 --workers=${NUM_WORKERS}",
        "env": {},
        "external": true,
        "gpu": false,
        "volumes": [],
        "allowed_addresses": [],
        "allowed_delegate_addresses": [],
        "allowed_ips": [],
        "accepted_payments": {},
        "generates_proofs": false
    },
    "inputs": [
        {
            "id": "ONNX_DEFAULT_MODEL_ID",
            "path": "env.ONNX_DEFAULT_MODEL_ID",
            "description": "The default model to preload on startup.",
            "type": "string",
            "required": false
        },
        {
            "id": "ONNX_CACHE_DIR",
            "path": "env.ONNX_CACHE_DIR",
            "description": "The directory to cache ONNX models in.",
            "type": "string",
            "required": false
        },
        {
            "id": "NUM_WORKERS",
            "path": "command#NUM_WORKERS",
            "description": "The number of workers to use with the server.",
            "type": "integer",
            "required": false,
            "default": 2
        }
    ]
}
