{
    "config": {
        "id": "onnx-inference:1.0.0",
        "image": "ritualnetwork/onnx_inference_service:1.0.0",
        "description": "ONNX Inference Service",
        "command": "--bind=0.0.0.0:3000 --workers=${NUM_WORKERS}",
        "env": {
            "MODEL_SOURCE": "${MODEL_SOURCE}",
            "LOAD_ARGS": "${LOAD_ARGS}",
            "TEST_ENV": "${TEST_ENV}"
        }
    },
    "inputs": [
        {
            "id": "MODEL_SOURCE",
            "description": "The source of the model to load. Used for preloading a model.",
            "type": "string",
            "required": false
        },
        {
            "id": "LOAD_ARGS",
            "description": "Arguments to load model with, as a stringified JSON object. Used for preloading a model.",
            "type": "string",
            "required": false
        },
        {
            "id": "TEST_ENV",
            "description": "Whether this is a testing instance.",
            "type": "string",
            "required": false,
            "default": "false"
        },
        {
            "id": "NUM_WORKERS",
            "description": "The number of workers to use with the server.",
            "type": "integer",
            "required": false,
            "default": 2
        }
    ]
}
